{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech enhancement by Wiener filtering - Voice activity detection and all pole modelling of the vocal tract\n",
    "*Laurent Colbois, Lionel Desarzens  \n",
    "COM-415 Audio and Acoustic Signal Processing course  \n",
    "EPFL 2018-2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Abstract***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review of existing techniques (How thorough ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithm description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote $y$ the noisy speech signal to enhance. $y$ is decomposed between a clean speech signal $s$ and a noise signal $n$, i.e. $y = s + n$. The denoising algorithm is supposed to build a good estimate $\\hat{s}$ of $s$. In practice, we build artificially $y$ by adding noise of our choice (either white noise, or noise from another sound file) to a clean speech sample. This allows in particular to control freely the SNR of $y$. The algorithm we use for speech enhancement is called *Iterative Wiener Filtering* and is presented hereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Wiener filtering\n",
    "The general approach for the denoising process is by the use of a Wiener filter that we apply in the STFT domain.\n",
    "The main principle of a Wiener filter is the following : given an input signal $x$ and a target signal $d$, the Wiener filter is a linear filter that, when applied on $x[n]$, gives as an output $\\hat{d}$ such that the error $e[n] = \\hat{d} - d$ is minimized in the least square sense. In the context of speech enhanchement, the input signal is $y = s + n$, and the target signal would be $s$. Note that in general $s$ is not actually known; however if one gains some insight on the spectral properties of $s$, a reasonable Wiener filter can be constructed.\n",
    "\n",
    "According to **Cite main article** or **Cite Loizou book**, in the context of speech enhancement a good Wiener filter, given in the frequency domain, is \n",
    "$$H(\\omega) = \\frac{P_s(\\omega)}{P_s(\\omega)+P_n(\\omega)}$$  \n",
    "where $P_s$ and $P_n$ are the power spectral density (PSD) of respectively the speech and the noise. The main focus of this work is to build a good estimation of those PSDs.  \n",
    "\n",
    "The general approach we choose to build those estimations consists in the following steps. \n",
    "> 0. Split the signal in analysis frames (possibly overlapping)\n",
    "> 1. Run the signal through a voice activity detection module (VAD), identify each frame as either speech or noise\n",
    "> 2. If processing a noise frame : compute a noise PSD estimation\n",
    "> 3. If processing a speech frame :  \n",
    "    4.1 Read the noise PSD estimation from previous noise frames  \n",
    "    4.2 Compute the speech PSD estimation  \n",
    "    4.3 Apply the denoising procedure (build and apply the Wiener filter)-> obtain estimate $\\hat{s}$ of $s$  \n",
    "    4.4 Optional : compute a noise PSD estimation for the current frame by considering the residual  $y - \\hat{s}$, that can possibly be considered when filtering future frames \n",
    "    \n",
    "The detail of how the implementation of the VAD algorithm, the noise PSD estimation and the speech PSD estimation are given in the following subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Voice Activity Detection (VAD)\n",
    "The VAD is an algorithm used in speech processing. It main purpose is (as said above) to classify a frame as containing speech, or not. Many different kind of VAD exist (see [], [] and []), but as the VAD is not the main objective of this project, we used the one defined in [] (see github... for the implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Estimating noise floor level [...]\n",
    "This VAD works using the average noise level. For each frame, the signal energy level is computed :\n",
    "\n",
    "$$L^{(n)} = \\sqrt{\\frac{1}{K} \\sum_{k=0}^{K-1} \\left(W_k \\cdot \\left \\lvert Y_{k}^{(n)} \\right \\rvert \\right )^2}$$\n",
    "\n",
    "where $W_k$ is a weigthing function, and $Y_k^{(n)}$ is the DFT of the $n$-th frame on K points. The aim of using a weighting function is to give more importance to the frequency where noise and signal are very different. <br>\n",
    "    As most of the energy of the noise is contained in the low part of the spectrum, using a high-pass filter might be a good idea. Moreover most of the speech energy is contained between $125 Hz$ and $300 Hz$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Dual-Constant time integrator\n",
    "For this method, the estimate of the noise floor level at $n$-th frame can be computed as follow :\n",
    "\n",
    "$$L_{min}^{(n)} = \\begin{cases} \\left (1-\\frac{T}{\\tau_{up}} \\right )L_{min}^{(n-1)}+\\frac{T}{\\tau_{up}}L^{n}, & L^{(n)}>L_{min}^{(n-1)} \\\\ \n",
    "\\left (1-\\frac{T}{\\tau_{down}} \\right )L_{min}^{(n-1)}+\\frac{T}{\\tau_{down}}L^{n}, & L^{(n)}\\leq L_{min}^{(n-1)} \\end{cases}$$\n",
    "\n",
    "where $T$ is the frame duration, $\\tau_{up}$ and $\\tau_{down}$ are the time constant to track the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Decision\n",
    "Finally, using the estimated noise floor level, and the signal energy level, the decision of the VAD can be expressed as follow :\n",
    "$$V^{(n)} = \\begin{cases} 0 & \\mbox{if} \\frac{L^{(n)}}{L_{min}^{(n)}}<T_{down} \\\\ \n",
    "1 & \\mbox{if} \\frac{L^{(n)}}{L_{min}^{(n)}}>T_{up} \\\\\n",
    "V^{(n-1)} & \\mbox{otherwise}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Further implementation\n",
    "It has been shown in [???] that the previous explained method works well on high SNR values. In some case, it might be interesting to implement a second type of VAD which is more robust to low SNR. Such algorithms are called Statistical VAD.\n",
    "\n",
    "As the aime of this project was to reduce noise, we did not implement this king of VAD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Noise PSD estimation\n",
    "All computations are done under the assumption that the noise is white centered gaussian with variance $\\sigma_n^2$. In this case we simply have $P_n(\\omega) = \\sigma_n^2$.\n",
    "\n",
    "**Update rule for $\\sigma$'s**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Speech PSD estimation : all-pole model\n",
    "The estimation of the speech PSD relies on modelling the speech production as an auto-regressive model. **Cite Lim, Oppenheim**. More specifically, the speech signal $s$ is written as \n",
    "$$ s[l] = \\sum\\limits_{k=1}^{p} a_k s[l-k] + g\\cdot w[l] $$  \n",
    "where $g$ is a gain factor, and $w[l]$ is a simple excitation. $p$ is a chosen parameter. For voiced speech, w[l] is modeled as a simple periodic excitation, while it is modeled as white noise for unvoiced speech. This is also equivalent to say the vocal tract acts on the initial exictation as the following transfer function :\n",
    "\n",
    "$$ V(\\omega) = \\frac{g}{1- \\sum\\limits_{k=1}^{p} a_k e^{-jk\\omega}} $$. In this case the PSD \n",
    "$$ P_s(\\omega) = \\frac{g^2}{\\left| 1- \\sum\\limits_{k=1}^{p} a_k e^{-jk\\omega} \\right|^2}$$\n",
    "$$ g^2 \\text{ such that } \\int_{-\\pi}^{\\pi} P_s(\\omega) \\text{d}\\omega = \\sum\\limits_{k=0}^{N-1}y^2[k] - N\\sigma_n^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics  \n",
    "An obvious qualitative way of evaluating the performance of a speech enhancement system is simply to listen to the signal before and after the denoising operation. However we would like to also have a quantitative measure of the efficiency of the algorithm. We choose the two following metrics :\n",
    "\n",
    "> - A posteriori SNR : from the original signal $y$ and speech signal estimation $\\hat{s}$, one can build an estimate $\\hat{n} = y - \\hat{s}$ of the noise signal. This allows to compute the SNR of $\\hat{s}$ with respect to $\\hat{n}$, and to compare it with the true SNR of $s$ with respect to $n$. This SNR a posteriori should be higher than the true SNR if the denoising operation is successful.    \n",
    "\n",
    "\n",
    "> - Intelligibility : even when it increases the SNR, the denoising operation might add some distortion in the speech signal and decrease its intelligibility. In order to get a quantitative measure of intelligibilty, we use a pretrained neural network for classification of speech signal (Google Speech Command Dataset). This network is asked to classify speech signals at various SNR ratios, and we compare its classification certainty for noisy speech and denoised speech. The code for this operation was greatly inspired from [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "\n",
    "<head>\n",
    "<title>evaluation</title>\n",
    "<meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\">\n",
    "<meta name=\"generator\" content=\"bibtex2html\">\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "\n",
    "<!-- This document was automatically generated with bibtex2html 1.98\n",
    "     (see http://www.lri.fr/~filliatr/bibtex2html/),\n",
    "     with the following command:\n",
    "     /usr/bin/bibtex2html evaluation.bib  -->\n",
    "\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr valign=\"top\">\n",
    "<td align=\"right\" class=\"bibtexnumber\">\n",
    "[<a name=\"Mermet:255834\">1</a>]\n",
    "</td>\n",
    "<td class=\"bibtexitem\">\n",
    "Alexis Mermet.\n",
    " Augmenting \"pyroomacoustics\" with machine learning utilities.\n",
    " 2018.\n",
    " SEMESTER_PROJECT.\n",
    "[<a href=\"http://infoscience.epfl.ch/record/255834\">http</a>&nbsp;]\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table><hr><p><em>This file was generated by\n",
    "<a href=\"http://www.lri.fr/~filliatr/bibtex2html/\">bibtex2html</a> 1.98.</em></p>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
